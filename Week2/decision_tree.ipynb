{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The big picture\n",
    "\n",
    "## 1. What is needed to build a decision tree (CART):\n",
    "                \n",
    "   **The principle of abstraction**\n",
    "\n",
    "1. Question --> A node is splitted based on the question asked.\n",
    "2. Split the data --> Based on the answer to the question, the data can be splitted into two new sets of data.\n",
    "3. metric --> Used to measure the homogeinity of a node, such as gini impurity or entropy.\n",
    "4. info_gain --> The change of the gini impurity or entropy before and after the split.(we need to maximize it to get the best node split)\n",
    "5. get_best_split --> Based on the info_gain of all possible split, we will pick the question that gives use the best split. \n",
    "6. Leaf --> The end node (where no more info gained). It should contain the predictions we want.\n",
    "7. Node --> a question can be asked to further lower and info gain and split into more nodes or leaves.\n",
    "\n",
    "\n",
    "![Decision_Tree](img/decision_tree.png)\n",
    "\n",
    "## 2. How are we going to structure our code:\n",
    "\n",
    "1. Package our decision tree classifier into a class, and the class should be able to do the following:\n",
    "\n",
    "    * train the model with training datasets.\n",
    "    * predict the result with the trained model and new data.\n",
    "    * some private methods to help build the class.\n",
    "    * simple print method to visualize the tree.\n",
    "\n",
    "\n",
    "2. How to represent the whole tree?\n",
    "\n",
    "    * The tree (model) is made of nodes and leaves. So we will construct node and leaf classes and use them to represent the tree (model).\n",
    "\n",
    "**Based on step 1, we can come up with the following structure:**\n",
    "\n",
    "Note: \n",
    "1. X is matrix (features for multiple samples) \n",
    "2. x is vector (feature for a single sample)\n",
    "3. y is vector (labels for all samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"\n",
    "    A node is splitted based on the question asked.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_number, feature):\n",
    "        self.column_number = column_number\n",
    "        self.feature = feature\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Some way to represent a question.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def ask(self, row):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            bool\n",
    "        \"\"\"  \n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Leaf:\n",
    "    \"\"\"\n",
    "    The end node (where no more info is gained). It should contain the predictions we want with only one type of label.\n",
    "    \"\"\"\n",
    "    def __init__(self, y):\n",
    "        self.prediction = self.y[0]\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Some way to represent a leaf.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def _label_counts(y):\n",
    "        \"\"\"\n",
    "        Counts the number of the samples that have the same label.\n",
    "        Return:\n",
    "            counts: dict\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A part of the tree. It is used to structure the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "    \n",
    "    \n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Decision tree classifier based on CART algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, metric=\"gini\"):\n",
    "        self.metric_type = metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree, in other words, train the model, and pass it to the class variable.\n",
    "        Recursion should be used with multiple returns. Return should be either a leaf or a node.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the model to predict the result with the new data X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def print_tree(self):\n",
    "        \"\"\"\n",
    "        It is a helper method to visualize the tree if the model is small. \n",
    "        \"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def _split(X, y, question):\n",
    "        \"\"\"\n",
    "        A private static helper function used to split the node into two nodes based on a question.\n",
    "        It is a static method because it does not need  the access to class variables.\n",
    "        But for the completeness of the algorithm, it would be better to still put it inside the class.\n",
    "            \n",
    "        Returns:\n",
    "            true_branch: np array\n",
    "            false_branch: np array\n",
    "        \"\"\"\n",
    "        pass\n",
    "            \n",
    "    def _get_metric(self, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            gini or entropy: float\n",
    "                a metric used in the information gain.\n",
    "        \"\"\"\n",
    "            \n",
    "        if self.metric_type == 'gini':\n",
    "            pass\n",
    "        if self.metric_type == 'entropy':\n",
    "            pass\n",
    "        \n",
    "    def _get_info_gain(self, y, true_branch, false_branch):\n",
    "        \"\"\"\n",
    "        Calculate the info gain after a node is splitted into two more nodes.\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'gini':\n",
    "            pass\n",
    "        if self.metric_type == 'entropy':\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            best_question: an instance of Question that get the highest info gain. We use it to split the tree.\n",
    "            best_gain: the info gain when the best question is asked.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work on individual pieces.\n",
    "\n",
    "Now we have the basic structure of the code, and most importantly, the big picture of how to build the decision tree, now it's time to work on small pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, make a test dataset for code testing.\n",
    "\n",
    "The dataset looks like this:\n",
    "\n",
    "```\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "The first feature is color, and the second is radius.\n",
    "The third column is label.\n",
    "\n",
    "However, since we are going to work with numpy array, it would be better to convert the categorical feature into nominal numbers.\n",
    "\n",
    "**For the color feature,**\n",
    "\n",
    "* 1 --> 'green'\n",
    "* 2 --> 'yellow'\n",
    "* 3 --> 'red'\n",
    "   \n",
    "**For the label,**\n",
    "\n",
    "* 1 --> 'apple'\n",
    "* 2 --> 'grape'\n",
    "* 3 --> 'lemon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array([\n",
    "    [1, 3, 1],\n",
    "    [2, 3, 1],\n",
    "    [3, 1, 2],\n",
    "    [3, 1, 2],\n",
    "    [2, 3, 3],\n",
    "])\n",
    "\n",
    "X = training_data[:, :-1]\n",
    "y = training_data[:, -1]\n",
    "\n",
    "# A list or numpy array does not have a header, \n",
    "# so for demonstration purpose, I manually added a header.\n",
    "header = [\"color\", \"diameter\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 3],\n",
       "       [3, 1],\n",
       "       [3, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Question class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"\n",
    "    A node is splitted based on the question asked.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_number, threshold):\n",
    "        self.column_number = column_number\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Is {} > {}\".format(\n",
    "            header[self.column_number], self.threshold\n",
    "        )\n",
    "    \n",
    "    def ask(self, x):\n",
    "        feature = x[self.column_number]\n",
    "        return feature > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is:  Is diameter > 1\n",
      "For the first test sample, the answer is:  True\n",
      "For the second test sample, the answer is:  False\n"
     ]
    }
   ],
   "source": [
    "# Test if it works\n",
    "\n",
    "sample_1 = training_data[1, :]\n",
    "sample_2 = training_data[2, :]\n",
    "q = Question(1, 1)\n",
    "print('The question is: ', q)\n",
    "print('For the first test sample, the answer is: ', q.ask(sample_1))\n",
    "print('For the second test sample, the answer is: ', q.ask(sample_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Leaf class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"\n",
    "    The end node (where no more info is gained). It should contain the predictions we want with only one type of label.\n",
    "    \"\"\"\n",
    "    def __init__(self, y):\n",
    "        self.prediction = self._label_counts(y)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.prediction)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _label_counts(y):\n",
    "        \"\"\"\n",
    "        Counts the number of the samples that have the same label.\n",
    "        Return:\n",
    "            counts: dict\n",
    "        \"\"\"\n",
    "        \n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This leaf contains the following prediction:  3\n"
     ]
    }
   ],
   "source": [
    "# Test if it works\n",
    "\n",
    "labels = np.array([3, 3, 1, 2, 1, 3])\n",
    "leaf = Leaf(labels)\n",
    "print('This leaf contains the following prediction: ', leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Node class.\n",
    "\n",
    "**Actually it is so simple that the class has already be completed during the structuring process.**\n",
    "\n",
    "A little bit more explanation:\n",
    "1. We need the question in the Node, because the question is more like a guide, pointing where the data should flow. (think about tensorflow).\n",
    "\n",
    "2. Every node has a true branch, and a false branch. Again, whether data flows into a true branch or a false branch all depends on the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A part of the tree. It is used to structure the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, question, true_node, false_node):\n",
    "        self.question = question\n",
    "        self.true_node = true_node\n",
    "        self.false_node = false_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the hardest part, let's build the classifier:\n",
    "Because it is a big class, you should build it step by step, following the order that we have come up with when we were structuring the code.\n",
    "\n",
    "1. Split data\n",
    "3. metric\n",
    "4. info_gain \n",
    "5. get_best_split\n",
    "\n",
    "All the functions above will be part of the model training process, which is the `fit` method in the class.\n",
    "\n",
    "Then, you can work on the following two methods:\n",
    "\n",
    "1. visualization --> a way to visualize the decision tree, and test the trained model.\n",
    "2. predict --> a way to predict new instance based on the trained model.\n",
    "\n",
    "\n",
    "**Remember, after you finish each method, you should test it to make sure it actually works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Decision tree classifier based on CART algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, metric=\"gini\"):\n",
    "        # hyperparameter\n",
    "        self.metric_type = metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree, in other words, train the model, and pass it to the class variable.\n",
    "        Recursion should be used with multiple returns. Return should be either a leaf or a node.\n",
    "        \"\"\"\n",
    "        gain, question = self._find_best_split(X, y)\n",
    "        \n",
    "        if gain == 0:\n",
    "            return Leaf(y)\n",
    "        \n",
    "        X_true_branch, y_true_branch, X_false_branch, y_false_branch = self._split(X, y, question)\n",
    "        \n",
    "        true_node = self.fit(X_true_branch, y_true_branch)\n",
    "        false_node = self.fit(X_false_branch, y_false_branch)\n",
    "        self.tree = Node(question, true_node, false_node)\n",
    "        return self.tree\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Use the model to predict the result with the new data X.\n",
    "        \"\"\"\n",
    "        return self._predict(self.tree, x)    \n",
    "        \n",
    "    def _predict(self, tree, x):\n",
    "        \"\"\"\n",
    "        Use the model to predict the result with the new data X.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(tree, Leaf):\n",
    "            return tree.prediction\n",
    "\n",
    "        if tree.question.ask(x):\n",
    "            return self._predict(tree.true_node, x)\n",
    "        else:\n",
    "            return self._predict(tree.false_node, x)\n",
    "          \n",
    "    def print_tree(self, spacing=''):\n",
    "        self._print_tree(self.tree, spacing=spacing)\n",
    "            \n",
    "            \n",
    "    def _print_tree(self, tree, spacing=''):\n",
    "        \"\"\"\n",
    "        It is a helper method to visualize the tree if the model is small. Used to test model. \n",
    "        \"\"\"\n",
    "        if isinstance(tree, Leaf):\n",
    "            print(spacing + \"Predict\", tree.prediction)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print(spacing + str(tree.question))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print(spacing + '--> True:')\n",
    "        self._print_tree(tree.true_node, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print(spacing + '--> False:')\n",
    "        self._print_tree(tree.false_node, spacing + \"  \")\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _split(X, y, question):\n",
    "        \"\"\"\n",
    "        A private static helper function used to split the node into two nodes based on a question.\n",
    "        It is a static method because it does not need  the access to class variables.\n",
    "        But for the completeness of the algorithm, it would be better to still put it inside the class.\n",
    "            \n",
    "        Returns:\n",
    "            true_branch: np array\n",
    "            false_branch: np array\n",
    "        \"\"\"\n",
    "        data = np.hstack((X, y.reshape(-1, 1)))\n",
    "        true_branch = np.array([])\n",
    "        false_branch = np.array([])\n",
    "        for row in data:\n",
    "            if question.ask(row):\n",
    "                true_branch = np.vstack((true_branch, row)) if true_branch.size else row.reshape(1, -1)\n",
    "            else:\n",
    "                false_branch = np.vstack((false_branch, row)) if false_branch.size else row.reshape(1, -1)\n",
    "        \n",
    "        if len(true_branch) == 0 or len(false_branch) == 0:\n",
    "            X_true_branch, y_true_branch, X_false_branch, y_false_branch = [], [], [], []\n",
    "        else:\n",
    "            X_true_branch = true_branch[:, :-1]\n",
    "            y_true_branch = true_branch[:, -1]\n",
    "            X_false_branch = false_branch[:, :-1]\n",
    "            y_false_branch = false_branch[:, -1]\n",
    "        \n",
    "        return X_true_branch, y_true_branch, X_false_branch, y_false_branch\n",
    "            \n",
    "    def _get_metric(self, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            gini or entropy: float\n",
    "                a metric used in the information gain.\n",
    "        \"\"\"\n",
    "        n_samples = len(y)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        if self.metric_type == 'gini':\n",
    "            impurity = 1\n",
    "            for count in counts:\n",
    "                probability = float(count / n_samples)\n",
    "                impurity -= probability ** 2\n",
    "            return impurity\n",
    "                \n",
    "        if self.metric_type == 'entropy':\n",
    "            entropy = 0\n",
    "            for count in counts:\n",
    "                probability = float(count / n_samples)\n",
    "                entropy -= probability * np.log2(probability)\n",
    "            return entropy\n",
    "            \n",
    "        \n",
    "    def _get_info_gain(self, y, y_true_branch, y_false_branch):\n",
    "        \"\"\"\n",
    "        Calculate the info gain after a node is splitted into two more nodes.\n",
    "        \"\"\"\n",
    "        p_true = len(y_true_branch) / len(y)\n",
    "        p_false = 1 - p_true\n",
    "        \n",
    "        metric_before_split = self._get_metric(y)\n",
    "        metric_true = self._get_metric(y_true_branch)\n",
    "        metric_false = self._get_metric(y_false_branch)\n",
    "        \n",
    "        gain = metric_before_split - p_true * metric_true - p_false * metric_false\n",
    "        \n",
    "        return gain\n",
    "        \n",
    "        \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            best_question: an instance of Question that get the highest info gain. \n",
    "                We use it to split the tree.\n",
    "            best_gain: the info gain when the best question is asked.\n",
    "        \"\"\"\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        n_features = len(X[0])\n",
    "        \n",
    "        for column_number in range(n_features):\n",
    "            thresholds = set([x[column_number] for x in X])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                question = Question(column_number, threshold)         \n",
    "                \n",
    "                X_true_branch, y_true_branch, X_false_branch, y_false_branch = self._split(X, y, question)\n",
    "                \n",
    "                if len(X_true_branch) == 0 or len(X_false_branch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._get_info_gain(y, y_true_branch, y_false_branch)\n",
    "                \n",
    "                if gain >= best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_question = question\n",
    "        \n",
    "        return best_gain, best_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test each method in DecisionTreeClassifier class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Test `_split` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_true_branch is <class 'numpy.ndarray'> \n",
      "[[1 3]\n",
      " [2 3]\n",
      " [2 3]]\n",
      "-------------------------\n",
      "y_true_branch is <class 'numpy.ndarray'> \n",
      "[1 1 3]\n",
      "-------------------------\n",
      "X_false_branch is <class 'numpy.ndarray'> \n",
      "[[3 1]\n",
      " [3 1]]\n",
      "-------------------------\n",
      "y_false_branch is <class 'numpy.ndarray'> \n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "question = Question(1, 1)\n",
    "X_true_branch, y_true_branch, X_false_branch, y_false_branch = decision._split(X, y, question)\n",
    "print('X_true_branch is {} \\n{}'.format(type(X_true_branch), X_true_branch))\n",
    "print('-------------------------')\n",
    "print('y_true_branch is {} \\n{}'.format(type(y_true_branch), y_true_branch))\n",
    "print('-------------------------')\n",
    "print('X_false_branch is {} \\n{}'.format(type(X_false_branch), X_false_branch))\n",
    "print('-------------------------')\n",
    "print('y_false_branch is {} \\n{}'.format(type(y_false_branch), y_false_branch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Test `_get_metric` method:\n",
    "$$\n",
    "impurity = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested impurities are \n",
      "0.0\n",
      "0.5\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "impurity_1 = decision._get_metric(np.array([1, 1, 1, 1, 1]))\n",
    "impurity_2 = decision._get_metric(np.array([1, 1, 2, 2]))\n",
    "impurity_3 = decision._get_metric(np.array([1, 1, 0, 0, 0]))\n",
    "print('The tested impurities are \\n{}\\n{}\\n{}'.format(\n",
    "    impurity_1, impurity_2, impurity_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "entropy = - \\sum_{i=1}^{n} p_i {\\log_2 p_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested impurities are \n",
      "0.0\n",
      "1.0\n",
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='entropy')\n",
    "\n",
    "impurity_1 = decision._get_metric(np.array([1, 1, 1, 1, 1]))\n",
    "impurity_2 = decision._get_metric(np.array([1, 1, 2, 2]))\n",
    "impurity_3 = decision._get_metric(np.array([1, 1, 0, 0, 0]))\n",
    "print('The tested impurities are \\n{}\\n{}\\n{}'.format(\n",
    "    impurity_1, impurity_2, impurity_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test `_get_info_gain` method:\n",
    "$$\n",
    "gain = metric_a - \\sum_{i=1}^{n} p_i * metric_b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The info gain is 0.37333333333333324\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "_, y_true_branch, _, y_false_branch = decision._split(X, y, q)\n",
    "\n",
    "gain = decision._get_info_gain(y, y_true_branch, y_false_branch)\n",
    "print('The info gain is {}'.format(gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Test `_find_best_split` method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the main node:\n",
      "0.37333333333333324\n",
      "Is diameter > 1\n",
      "-----------------\n",
      "after the first split\n",
      "The best gain for the true branch: 0.11111111111111116\n",
      "The best gain for the false branch: 0\n",
      "The best question for the true branch is: Is color > 1 \n",
      "The best question for the false branch is: None \n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "best_gain, best_question = decision._find_best_split(X, y)\n",
    "print('For the main node:')\n",
    "print(best_gain)\n",
    "print(best_question)\n",
    "print('-----------------')\n",
    "print('after the first split')\n",
    "\n",
    "X_true_branch, y_true_branch, X_false_branch, y_false_branch = decision._split(X, y, question)\n",
    "best_gain_true_branch, best_question_true_branch = decision._find_best_split(X_true_branch, y_true_branch)\n",
    "best_gain_false_branch, best_question_false_branch = decision._find_best_split(X_false_branch, y_false_branch)\n",
    "\n",
    "print('The best gain for the true branch: {}'.format(best_gain_true_branch))\n",
    "print('The best gain for the false branch: {}'.format(best_gain_false_branch))\n",
    "print('The best question for the true branch is: {} '.format(best_question_true_branch))\n",
    "print('The best question for the false branch is: {} '.format(best_question_false_branch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Test `fit` method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diameter > 1\n",
      "--> True:\n",
      "  Is color > 1\n",
      "  --> True:\n",
      "    Predict 1\n",
      "  --> False:\n",
      "    Predict 1\n",
      "--> False:\n",
      "  Predict 2\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "decision.fit(X, y)\n",
    "decision.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diameter > 1\n",
      "--> True:\n",
      "  Is color > 1\n",
      "  --> True:\n",
      "    Predict 1\n",
      "  --> False:\n",
      "    Predict 1\n",
      "--> False:\n",
      "  Predict 2\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='entropy')\n",
    "\n",
    "decision.fit(X, y)\n",
    "decision.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "decision.fit(X, y)\n",
    "y_predict = decision.predict(X[0])\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
